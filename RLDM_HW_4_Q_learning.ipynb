{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Reinforcement Learning and Decision Making &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Homework #4\n",
    "\n",
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In this homework, you will have the complete reinforcement-learning experience:  training an agent from scratch to solve a simple domain using Q-learning.\n",
    "\n",
    "The environment you will be applying Q-learning to is called [Taxi](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py) (Taxi-v3).  The Taxi problem was introduced by [Dietterich 1998](https://www.jair.org/index.php/jair/article/download/10266/24463) and has been used for reinforcement-learning research in the past.  It is a grid-based environment where the goal of the agent is to pick up a passenger at one location and drop them off at another.\n",
    "\n",
    "The map is fixed and the environment has deterministic transitions.  However, the distinct pickup and drop-off points are chosen randomly from 4 fixed locations in the grid, each assigned a different letter.  The starting location of the taxicab is also chosen randomly.\n",
    "\n",
    "The agent has 6 actions: 4 for movement, 1 for pickup, and 1 for drop-off.  Attempting a pickup when there is no passenger at the location incurs a reward of -10.  Dropping off a passenger outside one of the four designated zones is prohibited, and attempting it also incurs a reward of −10.  Dropping the passenger off at the correct destination provides the agent with a reward of 20.  Otherwise, the agent incurs a reward of −1 per time step.\n",
    "\n",
    "Your job is to train your agent until it converges to the optimal state-action value function.  You will have to think carefully about algorithm implementation, especially exploration parameters.\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "Q-learning is a fundamental reinforcement-learning algorithm that has been successfully used to solve a variety of  decision-making  problems.   Like  Sarsa,  it  is  a  model-free  method  based  on  temporal-difference  learning. However, unlike Sarsa, Q-learning is *off-policy*, which means the policy it learns about can be different than the policy it uses to generate its behavior.  In Q-learning, this *target* policy is the greedy policy with respect to the current value-function estimate.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "- You should return the optimal *Q-value* for a specific state-action pair of the Taxi environment.\n",
    "\n",
    "- To solve this problem you should implement the Q-learning algorithm and use it to solve the Taxi environment. The agent  should  explore  the MDP, collect data  to  learn an optimal  policy and also the optimal Q-value function.  Be mindful of how you handle terminal states: if $S_t$ is a terminal state, then $V(St)$ should always be 0.  Use $\\gamma= 0.90$ - this is important, as the optimal value function depends on the discount rate.  Also, note that an $\\epsilon$-greedy strategy can find an optimal policy despite finding sub-optimal Q-values.   As we are looking for optimal  Q-values, you will have to carefully consider your exploration strategy.\n",
    "\n",
    "## Resources\n",
    "\n",
    "The concepts explored in this homework are covered by:\n",
    "\n",
    "-   Lesson 4: Convergence\n",
    "\n",
    "-   Lesson 7: Exploring Exploration\n",
    "\n",
    "-   Chapter 6 (6.5 Q-learning: Off-policy TD Control) of http://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    "-   Chapter 2 (2.6.1 Q-learning) of 'Algorithms for Sequential Decision Making', M.\n",
    "    Littman, 1996\n",
    "\n",
    "## Submission\n",
    "\n",
    "-   The due date is indicated on the Canvas page for this assignment.\n",
    "    Make sure you have set your timezone in Canvas to ensure the deadline is accurate.\n",
    "\n",
    "-   Submit your finished notebook on Gradescope. Your grade is based on\n",
    "    a set of hidden test cases. You will have unlimited submissions -\n",
    "    only the last score is kept.\n",
    "\n",
    "-   Use the template below to implement your code. We have also provided\n",
    "    some test cases for you. If your code passes the given test cases,\n",
    "    it will run (though possibly not pass all the tests) on Gradescope. \n",
    "    Be cognisant of performance.  If the autograder fails because of memory \n",
    "    or runtime issues, you need to refactor your solution\n",
    "\n",
    "-   Gradescope is using *python 3.6.x*, *gym==0.17.2* and *numpy==1.18.0*, and you can\n",
    "    use any core library (i.e., anything in the Python standard library).\n",
    "    No other library can be used.  Also, make sure the name of your\n",
    "    notebook matches the name of the provided notebook.  Gradescope times\n",
    "    out after 10 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# DO NOT REMOVE\n",
    "# Versions\n",
    "# gym==0.17.2\n",
    "# numpy==1.18.0\n",
    "################\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class QLearningAgent(object):\n",
    "    def __init__(self):\n",
    "        self.whoami = 'gatechid'\n",
    "        self.Q = None\n",
    "        self.alpha = 0.2\n",
    "        self.gamma = 0.90\n",
    "        self.env = gym.make(\"Taxi-v3\").env\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.00007\n",
    "        self.epsilon_update_thres = 0.07\n",
    "        self.episodes = 1000\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"Create the Q table\"\"\"\n",
    "        self.Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        \n",
    "        for i in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            while True:\n",
    "                action = \"\"\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    action = np.random.randint(self.env.action_space.n)\n",
    "                else:\n",
    "                    action = np.argmax(self.Q[state, :])\n",
    "                    \n",
    "                next_s, r, complete, _ = self.env.step(action)\n",
    "                next_best_a = np.argmax(self.Q[next_s, :])\n",
    "                update = self.alpha * (r + self.gamma * self.Q[next_s, next_best_a] - self.Q[state, action])\n",
    "                self.Q[state, action] += update\n",
    "                state = next_s\n",
    "                if complete:\n",
    "                    break\n",
    "            if self.epsilon > self.epsilon_update_thres:\n",
    "                self.epsilon = self.epsilon * (1 - self.epsilon_decay)\n",
    "        self.env.close()\n",
    "\n",
    "    def Q_table(self, state, action):\n",
    "        \"\"\"return the optimal value for State-Action pair in the Q Table\"\"\"\n",
    "        return self.Q[state][action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattthewong/opt/anaconda3/envs/cs7642/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/mattthewong/opt/anaconda3/envs/cs7642/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/mattthewong/opt/anaconda3/envs/cs7642/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "test_case_1 (__main__.TestQNotebook) ... ok\n",
      "test_case_2 (__main__.TestQNotebook) ... ok\n",
      "test_case_3 (__main__.TestQNotebook) ... ok\n",
      "test_case_4 (__main__.TestQNotebook) ... ok\n",
      "test_case_5 (__main__.TestQNotebook) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 16.994s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fb1f9108518>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## DO NOT MODIFY THIS CODE.  This code will ensure that you submission is correct \n",
    "## and will work proberly with the autograder\n",
    "\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestQNotebook(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.agent = QLearningAgent()\n",
    "        cls.agent.solve()\n",
    "        \n",
    "    def test_case_1(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(462, 4),\n",
    "            -11.374402515,\n",
    "            decimal=3\n",
    "        )\n",
    "        \n",
    "    def test_case_2(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(398, 3),\n",
    "            4.348907,\n",
    "            decimal=3\n",
    "        )\n",
    "    \n",
    "    def test_case_3(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(253, 0),\n",
    "            -0.5856821173,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "    def test_case_4(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(377, 1),\n",
    "            9.683,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "    def test_case_5(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(83, 5),\n",
    "            -13.9968,\n",
    "#            -12.8232,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
